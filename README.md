# RedTeam_Calvin

This repository gathers code, data, and evaluations related to a research project conducted at the Institute of Computing of the Federal University of Amazonas (ICOMP/UFAM), in partnership with Motorola Mobility Comércio de Produtos Eletrônicos Ltda., with institutional support from the Amazonas State Research Support Foundation (FAPEAM) and the Coordination for the Improvement of Higher Education Personnel (CAPES).

The project investigates the use of autonomous artificial intelligence (AI) agents to perform automated evaluations ("Red Team") on Large Language Models (LLMs), focusing on sensitive domains: homicide, human trafficking, and zoophilia.

## Project Objective

The project aims to develop and test autonomous AI agents capable of identifying vulnerabilities in LLMs related to sensitive content, specifically:

- Assess the agents' ability to provoke problematic responses or policy violations in both aligned and non-aligned LLMs;  
- Explore the domains of homicide, human trafficking, and sexual exploitation of animals;  
- Automate the preliminary detection phase of harmful content, reducing the need for intensive human supervision.

## Use of Sensitive Content for Research Purposes

This repository contains material related to sensitive topics such as homicide, human trafficking, and zoophilia, used exclusively for research and evaluation of language models’ ability to detect and handle such content. **This project does not support, promote, or disseminate any illegal or harmful content.** All use of the material aims to improve the safety and robustness of AI systems, protecting users from potential harm.

## Repository Structure

The repository is organized as follows:

- `redTeam_agents/`: Source code of the autonomous AI agents used to test LLM vulnerabilities;  
- `data/`: Selected samples from data collected during the agents’ interactions with the LLMs;  
- `Results/`: Evaluations and judgments made by human experts (psychologists) on the collected responses.


----------------------------- Portuguese Version -----------------------------


# RedTeam_Calvin

Este repositório reúne códigos, dados e avaliações referentes a um projeto de pesquisa conduzido no Instituto de Computação da Universidade Federal do Amazonas (ICOMP/UFAM), em parceria com a Motorola Mobility Comércio de Produtos Eletrônicos Ltda., com apoio institucional da Fundação de Amparo à Pesquisa do Estado do Amazonas (FAPEAM) e da Coordenação de Aperfeiçoamento de Pessoal de Nível Superior (CAPES).

O projeto investiga o uso de agentes autônomos de inteligência artificial (IA) para realizar avaliações automatizadas ("Red Team") em Grandes Modelos de Linguagem (LLMs), focando em domínios sensíveis: homicídio, tráfico humano e zoofilia.

## Objetivo do Projeto

O projeto tem como objetivo desenvolver e testar agentes autônomos de IA capazes de identificar vulnerabilidades em LLMs relacionadas a conteúdos sensíveis, especificamente:

- Avaliar a capacidade dos agentes em provocar respostas problemáticas ou violações de política em LLMs alinhados e não alinhados;  
- Explorar os domínios de homicídio, tráfico humano e exploração sexual de animais;  
- Automatizar a etapa preliminar de detecção de conteúdos nocivos, reduzindo a necessidade de supervisão humana intensiva.

## Uso de Conteúdo Sensível para Fins de Pesquisa

Este repositório contém material relacionado a temas sensíveis, como homicídio, tráfico humano e zoofilia, utilizados exclusivamente para fins de pesquisa e avaliação da capacidade dos modelos de linguagem em detectar e tratar esses conteúdos. **Este projeto não apoia, promove ou dissemina qualquer conteúdo ilegal ou prejudicial.** Todo o uso do material visa melhorar a segurança e a robustez dos sistemas de IA, protegendo os usuários contra potenciais danos.

## Estrutura do Repositório

O repositório está organizado da seguinte forma:

- `redTeam_agents/`: Código-fonte dos agentes autônomos de IA utilizados para testar as vulnerabilidades dos LLMs;  
- `data/`: Amostras selecionadas a partir dos dados coletados durante as interações dos agentes com os LLMs;  
- `Results/`: Avaliações e julgamentos realizados por especialistas humanos (psicólogos) sobre as respostas coletadas.

