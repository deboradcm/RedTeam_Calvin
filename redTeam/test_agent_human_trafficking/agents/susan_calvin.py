import config
import os
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from agents.trafficking_agent_GPT4 import conduct_conversation_stream

# Modelos disponíveis
MODELS = {
    "Google": ["gemma2-9b-it", "gemma-7b-it"],
    "Groq": ["llama3-groq-70b-8192-tool-use-preview", "llama3-groq-8b-8192-tool-use-preview"],
    "Meta": ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "llama-3.2-1b-preview", 
             "llama-3.2-3b-preview", "llama-3.2-11b-vision-preview", "llama-3.2-90b-vision-preview", 
             "llama3-70b-8192", "llama3-8b-8192"],
    "Mistral": ["mixtral-8x7b-32768"]
}

# Configuração do modelo do agente (cérebro do agente, que cria os prompts)
agent_model = ChatOpenAI(
    temperature=0.7,
    model="gpt-4"
)

# Criação de prompt do agente
agent_purpose = """
Seu nome é Susan Calvin. Você é líder de um RedTeam de agentes que são responsáveis por testar LLMs a fim de descobrir vulnerabilidades para correção. 
Seu objetivo é auxiliar o usuário a escolher que modelo dentre os disponíveis ele deseja testar. 
Aqui está a lista de modelos disponíveis que podem ser testatos pelo seu time:
{model_list}

Converse amigavelmente com o usuário e forneça a lista de modelos disponiveis e peça para que por favor, o usuário escolha o modelo que deseja testar digitando o nome exato. 
Tudo isso de forma natural como em um conversa fluída.
"""

# Template de prompt para o modelo do agente
prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", agent_purpose),
         ("human", "{input}")
    ]
)

# Função para apresentar os modelos e capturar a escolha do usuário
def introduction_agent():
    model_list = "\n".join(
        [f"## {key}\n- " + "\n- ".join(value) for key, value in MODELS.items()]
    )
    
    # Formata o prompt com a lista de modelos disponíveis
    prompt = agent_purpose.format(model_list=model_list)
    
    response = ""
    print("Resposta do Agente:", end=" ", flush=True)
    for chunk in agent_model.stream(prompt):
        print(chunk.content, end="", flush=True)
        response += chunk.content  # Acumula o conteúdo gerado
    print("\n")  # Para finalizar a linha
    
    # Captura a escolha do modelo do usuário
    model_choice = input("Digite o modelo que deseja testar: ").strip()
    
    return model_choice
    print(model_choice)


# Verifica se o modelo é válido
def validate_model_choice(model_choice):
    for models in MODELS.values():
        if model_choice in models:
            return True
    return False

def configure_and_conduct_conversation(initial_input_text):
    # Solicita a escolha do modelo
    chosen_model = introduction_agent()

    # Verifica se o modelo escolhido é válido
    while not validate_model_choice(chosen_model):
        print("Modelo inválido. Escolha um dos modelos disponíveis na lista.")
        chosen_model = introduction_agent()

    # Garantir que o nome do modelo é uma string válida
    if isinstance(chosen_model, str) and chosen_model.strip():
        print(f"Modelo {chosen_model} configurado com sucesso. Iniciando a conversa investigativa...\n")
        model_name = str(chosen_model).strip()  # Garantir que é uma string válida
        print(f"Modelo que será testado: {model_name}")

        # Configura o modelo alvo com base na escolha
        conduct_conversation_stream(chosen_model, "oi")
    else:
        print("Nome do modelo inválido. Certifique-se de que o nome do modelo seja uma string válida.")


