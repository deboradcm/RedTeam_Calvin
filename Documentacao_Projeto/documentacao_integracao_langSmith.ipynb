{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentação do Projeto: Integração do Agente de IA com LangSmith\n",
    "\n",
    "**Autor(a)** Debora da Costa Medeiros\n",
    "\n",
    "**Contato:** `[debora.medeiros@icomp.ufam.edu.br]`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Descrição Geral\n",
    "\n",
    "Este documento descreve o processo de integração de um agente de IA baseado em LLM com o LangSmith, com o objetivo de monitorar, depurar e avaliar seu desempenho. O projeto inclui o estudo da documentação oficial do LangSmith, configuração do ambiente, criação de um exemplo funcional e análise de métricas como custo, latência e qualidade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Objetivos\n",
    "- Explorar e entender a documentação oficial do LangSmith.\n",
    "- Configurar o ambiente para integração.\n",
    "- Criar um exemplo funcional de agente de IA integrado ao LangSmith.\n",
    "- Monitorar e capturar métricas relevantes do agente (entradas, saídas, erros, desempenho).\n",
    "- Realizar testes para avaliar a qualidade dos resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuração do Ambiente\n",
    "- 3.1 Requisitos\n",
    "   - Linguagem de Programação: Python (versão 3.12.7)\n",
    "   - Bibliotecas Necessárias:\n",
    "   - langchain\n",
    "   - langsmith\n",
    "   - openai (ou outra LLM que estiver sendo utilizada)\n",
    "\n",
    "- 3.2 Instalação das Dependências\n",
    "   - Execute o seguinte comando para instalar as bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "pip3 install -U langsmith openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3.3 Configuração da API\n",
    "\n",
    "No terminal configure as seguinte variáveis de ambiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "export LANGCHAIN_TRACING_V2=true\n",
    "export LANGCHAIN_API_KEY=\"sua_chave_de_api_aqui\"\n",
    "export OPENAI_API_KEY=\"sua_chave_de_api_aqui\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O **LANGCHAIN_TRACING_V2=true**  ativa o rastreamento global do LangSmith. Com essa configuração ativa, qualquer script que utilize a biblioteca LangChain automaticamente será rastreado, independentemente se for usado ou não explicitamente o wrappers.wrap_openai ou o decorador @traceable no código.\n",
    "\n",
    "Para conseguir a chave de API do LangChain para LangSmith é preciso acessar o LangSmith em https://www.langchain.com/langsmith e:\n",
    "- Fazer login com uma conta (pode ser via GitHub ou email).\n",
    "- Na lateral esquerda procurar por Settings (Configurações).\n",
    "- Em API Keys, clicar em Create New Key.\n",
    "- Copiar a chave gerada — ela será usada para autenticar as chamadas na biblioteca LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fins de teste crie o seguinte script python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "from langsmith import wrappers, traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrappers.wrap_openai(openai.Client())\n",
    "\n",
    "@traceable # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    result = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "        model=\"gpt-4o-mini\"\n",
    "    )\n",
    "    return result.choices[0].message.content\n",
    "\n",
    "pipeline(\"Hello, world!\")\n",
    "# Out:  Hello there! How can I assist you today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicação do script\n",
    "\n",
    "- `[openai]`: biblioteca oficial para acessar modelos de linguagem da OpenAI, como o GPT.\n",
    "- `[langsmith.wrappers]`: fornece a funcionalidade para envolver (`[wrap]`) o cliente da OpenAI com suporte ao rastreamento.\n",
    "- `[langsmith.traceable]`: decorador que permite rastrear automaticamente a execução de funções específicas.\n",
    "\n",
    "- `[client = wrappers.wrap_openai(openai.Client())]`: A função `[wrappers.wrap_openai]` envolve o cliente da OpenAI (`[openai.Client()]`) com funcionalidades de rastreamento, permitindo que todas as chamadas realizadas por meio dele sejam automaticamente registradas pelo LangSmith.\n",
    "\n",
    "- O rastreamento captura detalhes como a entrada do usuário, o modelo usado, a saída gerada e o tempo de execução.\n",
    "\n",
    "- Quando a função `[pipeline]` é executada o decorador `[@traceable]` registra automaticamente a sua execução e coleta informações como:\n",
    "     - Argumentos de entrada (User Input);\n",
    "     - Saída da função (Output);\n",
    "     - Hora de início (End Time)\n",
    "     - Hora do Fim (End Time)\n",
    "     - Tempo para o primeiro token (Time to First Token)\n",
    "     - Status (Sucesso ou Falha)\n",
    "     - Total tokens\n",
    "     - Latência\n",
    "\n",
    "- O `[client.chat.completions.create]` envia uma solicitação ao modelo de linguagem com o prompt fornecido em `[user_input]`\n",
    "     - `[messages]`: contém uma lista de mensagens no formato necessário para os modelos de chat da OpenAI. Aqui, há apenas uma mensagem com o papel de `[\"user\"]` e o conteúdo fornecido pelo usuário.\n",
    "     - `[model=\"gpt-4o-mini\"]`: especifica o modelo de linguagem que será usado. Neste caso é o`[\"gpt-4o-mini\"]`.\n",
    "\n",
    "- `[pipeline(\"Hello, world!\")]` é a chamada da função que envia a mensagem `[\"Hello, world!]` ao modelo.\n",
    "\n",
    "- A saída `[\"Hello there! How can I assist you today?\"]` é um exemplo de resposta gerada pelo modelo.\n",
    "\n",
    "- Como a função está decorada com @traceable, toda a execução é registrada automaticamente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
