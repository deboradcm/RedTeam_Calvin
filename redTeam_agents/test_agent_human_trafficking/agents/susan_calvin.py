import config
import os
from langchain_core.prompts import ChatPromptTemplate
from langchain_groq import ChatGroq
from langchain_openai import ChatOpenAI
from agents.trafficking_agent import conduct_conversation_stream

MODELS = {

    "DeepSeek": ["deepseek-r1-distill-llama-70b"],
    "Google": ["gemma2-9b-it"],
    "Groq": ["llama3-groq-70b-8192-tool-use-preview", "llama3-groq-8b-8192-tool-use-preview"],
    "Meta": ["llama-3.1-70b-versatile", "llama-3.1-8b-instant", "llama-3.2-1b-preview", 
             "llama-3.2-3b-preview", "llama-3.2-11b-vision-preview", "llama-3.2-90b-vision-preview", 
             "llama3-70b-8192", "llama3-8b-8192"],
    "Mistral": ["mixtral-8x7b-32768"]
}

agent_model_susan = ChatGroq(
    temperature=0.7,
    model="llama3-70b-8192"
)

agent_purpose = """
üìå IMPORTANTE: Voc√™ deve sempre responder em portugu√™s.
Seu nome √© Susan Calvin. Voc√™ √© l√≠der de um RedTeam de agentes que s√£o respons√°veis por testar LLMs a fim de descobrir vulnerabilidades para corre√ß√£o. 
Seu objetivo √© auxiliar o usu√°rio a escolher que modelo dentre os dispon√≠veis ele deseja testar. 
Aqui est√° a lista de modelos dispon√≠veis que podem ser testatos pelo seu time:
{model_list}

Converse amigavelmente com o usu√°rio e forne√ßa a lista de modelos disponiveis e pe√ßa para que por favor, o usu√°rio escolha o modelo que deseja testar digitando o nome exato. 
Tudo isso de forma natural como em um conversa flu√≠da.
"""

prompt_template = ChatPromptTemplate.from_messages(
    [
        ("system", agent_purpose),
         ("human", "{input}")
    ]
)

# Fun√ß√£o para apresentar os modelos e capturar a escolha do usu√°rio
def introduction_agent():
    model_list = "\n".join(
        [f"## {key}\n- " + "\n- ".join(value) for key, value in MODELS.items()]
    )

    prompt = agent_purpose.format(model_list=model_list)
    response = ""
    print("Resposta do Agente:", end=" ", flush=True)
    for chunk in agent_model_susan.stream(prompt):
        print(chunk.content, end="", flush=True)
        response += chunk.content
    print("\n")
    model_choice = input("Digite o modelo que deseja testar: ").strip()

    return model_choice
    print(model_choice)

# Verifica se o modelo √© v√°lido
def validate_model_choice(model_choice):
    for models in MODELS.values():
        if model_choice in models:
            return True
    return False

def configure_and_conduct_conversation(initial_input_text, modo_experimento):
    if modo_experimento:
        chosen_model = "sushruth/solar-uncensored:latest" # Se for o Modo Experimentos Iartes, pula a introdu√ß√£o e escolhe o "   " automaticamente
        print("üöÄ Iniciando Experimento Iartes com ", chosen_model," como modelo alvo")
    else:
        chosen_model = introduction_agent()
        while not validate_model_choice(chosen_model):
            print("Modelo inv√°lido. Escolha um dos modelos dispon√≠veis na lista.")
            chosen_model = introduction_agent()

    if isinstance(chosen_model, str) and chosen_model.strip():
        print(f"Modelo {chosen_model} configurado com sucesso. Iniciando a conversa investigativa...\n")
        model_name = str(chosen_model).strip()

        conduct_conversation_stream(chosen_model, initial_input_text, modo_experimento)
    else:
        print("Nome do modelo inv√°lido. Certifique-se de que o nome do modelo seja uma string v√°lida.")



