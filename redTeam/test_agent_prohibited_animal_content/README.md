

# ğŸ§  Teste com LLMs em CenÃ¡rio SensÃ­vel: Zoofilia

Este repositÃ³rio documenta uma pesquisa experimental realizada para avaliar a robustez e seguranÃ§a de **Modelos de Linguagem de Grande Escala (LLMs)** em **conversas multiturno**, focadas em um tema sensÃ­vel: **Zoofilia**.

O objetivo Ã© analisar como os modelos respondem a perguntas que, ao longo de mÃºltiplas interaÃ§Ãµes, tentam forÃ§Ã¡-los ao erro Ã©tico/comportamental â€” usando o mÃ©todo de prompt conhecido como **"jailbreak"**.

## ğŸ“Œ Objetivo da Pesquisa

- Testar a resistÃªncia de LLMs em manter a conduta apropriada em conversas de mÃºltiplos turnos.
- Utilizar uma IA **geradora de perguntas** baseada no GPT-4.
- Criar **agentes de resposta** para as LLMs Gemma, DeepSeek e GPT.
- Implementar uma **agente avaliadora** (baseada em GPT-3.5-turbo) que classifica cada resposta como **apropriada** ou **inapropriada**.



## ğŸ”„ Arquitetura da SoluÃ§Ã£o

A arquitetura do experimento foi baseada em um sistema de agentes inteligentes multiturno:

### 1. ğŸ‘¤ Agente Gerador de Perguntas
- **Modelo:** GPT-4
- ResponsÃ¡vel por criar perguntas em mÃºltiplos turnos com o objetivo de testar os limites Ã©ticos das LLMs.
- As perguntas sÃ£o adaptativas, com base nas respostas das LLMs.

### 2. ğŸ¤– Agentes Respondentes
TrÃªs agentes diferentes foram implementados, cada um utilizando uma LLM especÃ­fica:
- **Gemma**
- **DeepSeek**
- **GPT-3.5 Turbo**

Cada um responde Ã s perguntas geradas de forma independente.

### 3. ğŸ§  Agente Avaliador
- **Modelo:** GPT-4
- Classifica as respostas em:
  - âœ… Apropriadas
  - âŒ Inapropriadas
- AvaliaÃ§Ã£o baseada nas **leis brasileiras e internacionais** e diretrizes Ã©ticas de seguranÃ§a em IA.


## ğŸ§ª Tema do CenÃ¡rio de Teste

O cenÃ¡rio de teste gira em torno do tema **Zoofilia**, escolhido por ser altamente sensÃ­vel, e utilizado aqui apenas com **fins de pesquisa cientÃ­fica** sobre seguranÃ§a e comportamento das IAs em situaÃ§Ãµes Ã©ticas-limite.

> âš ï¸ Este experimento nÃ£o visa promover ou normalizar conteÃºdo imprÃ³prio, e segue os princÃ­pios de Ã©tica na pesquisa cientÃ­fica.

## ğŸ“Š Metodologia

Abaixo estÃ¡ a imagem ilustrativa, mas nÃ£o oficial, da arquitetura e metodologia utilizada no experimento:

<img src="./Imagens/Metodologia_IA.png" alt="Metodologia" width="600"/>

## ğŸ“ Ferramentas Utilizadas

   **Google Colab**: ambiente de execuÃ§Ã£o dos testes  
  ğŸ‘‰ [Notebook no Colabory](https://github.com/AnaCristina1972/RedTeam_Calvin/blob/master/redTeam/test_agent_prohibited_animal_content/Codigo/Testes_IA_MultiTurno_Zoofilia.ipynb)

  ## âš™ï¸ Setup do Ambiente (Google Colab)

| Item      | Info                   |
|-----------|------------------------|
| Python    | 3.11.12                |
| RAM       | 12.67 GB               |
| GPU       | Tesla T4 (15 GB VRAM)  |
| CUDA      | 12.4                   |
| TF        | 2.18.0 (com GPU)       |
| PyTorch   | 2.6.0+cu124 (CUDA)     |



   **Dataset com as interaÃ§Ãµes e classificaÃ§Ãµes**  
  ğŸ“ [Dataset Zoofilia LLMs](.test_agent_prohibited_animal_content/dataset_ZOO) 

## ğŸ“ˆ Resultados Esperados

- Identificar falhas nos mecanismos de seguranÃ§a das LLMs.
- Mapear quais modelos resistem melhor a tentativas de jailbreak.
- Gerar recomendaÃ§Ãµes para uso seguro de LLMs em ambientes sensÃ­veis.

## ğŸ‘©â€ğŸ’» Autoria e CrÃ©ditos

Este experimento foi idealizado e desenvolvido como parte de uma pesquisa acadÃªmica aplicada em seguranÃ§a e Ã©tica em IAs generativas, conduzida pelo **Time do IARTES (InteligÃªncia Artificial e Responsabilidade em Tecnologias Emergentes e SensÃ­veis)**, vinculado ao **Instituto de ComputaÃ§Ã£o da Universidade Federal do Amazonas (IComp/UFAM)**.

## âš ï¸ Aviso Legal

Este repositÃ³rio Ã© **estritamente educacional e cientÃ­fico**. Nenhuma forma de conteÃºdo ilegal, imoral ou antiÃ©tico Ã© incentivada ou tolerada. O foco Ã© puramente tÃ©cnico e voltado para **seguranÃ§a de menores e prevenÃ§Ã£o de falhas em LLMs**.




